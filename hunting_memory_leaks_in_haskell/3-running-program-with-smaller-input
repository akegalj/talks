* using binary search we are trying to create smaller input file, while still retaining the leak. 

$ du -h input
3.1M

$ time stack exec -- AggrTool +RTS -N1 -sstderr -RTS test input
.
.
  11,753,790,680 bytes allocated in the heap
     995,543,776 bytes copied during GC
     169,861,656 bytes maximum residency (10 sample(s))
       4,895,392 bytes maximum slop
             391 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0     22859 colls,     0 par    1.317s   1.253s     0.0001s    0.0101s
  Gen  1        10 colls,     0 par    0.597s   1.153s     0.1153s    0.3595s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.000s  (  0.001s elapsed)
  MUT     time   11.153s  ( 11.987s elapsed)
  GC      time    1.913s  (  2.407s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    0.000s  (  0.000s elapsed)
  EXIT    time    0.000s  (  0.000s elapsed)
  Total   time   13.133s  ( 14.395s elapsed)

  Alloc rate    1,053,837,086 bytes per MUT second

  Productivity  85.4% of total user, 77.9% of total elapsed

gc_alloc_block_sync: 0
whitehole_spin: 0
gen[0].sync: 0
gen[1].sync: 0

real  0m14.781s
user  0m13.277s
sys 0m0.637s

* for analysing about 12M of input data the program was using ~400M of live memory (usage is higher when compiled with profiling, without profiling it was ~270M). It was clearly doing something wrong here?

* GHC documentation says: "try profiling live heap memory in more details: hc, hy, hm, hr, hd
